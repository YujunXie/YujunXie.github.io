PyTorch是一个基于python的科学计算包，作为一个高灵活性、速度快的深度学习平台，主要是提供了两个核心的功能特性：
- 一个类似于numpy的n维张量，可以利用GPU的性能进行计算
- 搭建和训练神经网络时的自动微分/求导机制

---

- [Tensor](#Tensor)
- [Autograd](#Autograd)
- [NN](#NN)
- [数据处理](#数据处理)

Tensor
---
张量类似于NumPy的`ndarray`，但还可以在GPU上使用来加速计算。

```python
import torch

x = torch.empty(5, 3)
x = torch.rand(5, 3)
x = torch.zeros(5, 3, dtype=torch.long)
x = torch.tensor([5.5, 3])  #从数据构造张量
#根据已有的tensor建立新的tensor。除非用户提供新的值，否则这些方法将重用输入张量的属性，例如dtype等
x = x.new_ones(5, 3, dtype=torch.double)
x = torch.randn_like(x, dtype=torch.float) 
x.size()
y = torch.rand(5, 3)
x + y || torch.add(x, y)
torch.add(x, y, out=result)
y.add_(x) #原位操作
z = x.view(-1, 8) #改变形状
x.item() #得到python数值

#与Numpy的变换
z = x.numpy()
x = torch.from_numpy(z)

#cuda
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor
    x = x.to(device)                       # 或者使用`.to("cuda")`方法
```

**Torch张量和NumPy数组将共享它们的底层内存位置，更改一个将更改另一个。**
**张量可以使用.to方法移动到任何设备（device）上**

超过100中`tensor`的运算操作，包括转置，索引，切片，数学运算， 线性代数，随机数等，具体访问[这里](https://pytorch.org/docs/stable/torch.html)

Autograd
---

autograd包使用**自动微分**来自动完成神经网络中**反向传播**的计算。当使用autograd时，网络前向传播将定义一个计算图；图中的节点是tensor，边是函数，这些函数是输出tensor到输入tensor的映射。这张计算图使得在网络中反向传播时梯度的计算十分简单。

设置`Tensor`的属性 `.requires_grad`为`True`，那么它将会追踪对于该张量的所有操作，所有操作都将构造一个计算图(**动态图**)，从而允许我们稍后在图中执行反向传播。当完成计算后可以通过调用`.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性.

要阻止一个张量被跟踪历史，可以调用`.detach()`方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。

为了防止跟踪历史记录（和使用内存），可以将代码块包装在`with torch.no_grad():`中。在评估模型时特别有用，因为模型可能具有`requires_grad = True`的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。

还有一个类对于autograd的实现非常重要：`Function`。

`Tensor`和`Function`互相连接生成了一个非循环图，它编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，它引用了一个创建了这个Tensor的Function（除非这个张量是用户手动创建的，即这个张量的grad_fn是`None`）。

```python
import torch

x = torch.ones(2, 2, requires_grad=True)
y = x + 2
print(y.grad_fn)
#<AddBackward0 object at 0x7f1b248453c8>
z = y * y * 3
out = z.mean()
out.backward()
#计算雅可比向量积得到梯度
print(x.grad)
#tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])

```
`.requires_grad_(...)` 原地改变了现有张量的 `requires_grad` 标志。如果没有指定的话，默认输入的这个标志是`False`。

NN
---
使用`torch.nn`包来构建神经网络。nn包依赖于autograd包来定义模型并对它们求导。一个`nn.Module`包含各个层和一个`forward(input)`方法，该方法返回output。

一个神经网络的典型训练过程如下：

- 定义包含一些可学习参数（或者叫权重）的神经网络
- 在输入数据集上迭代
- 通过网络处理输入
- 计算损失（输出和正确答案的距离）
- 将梯度反向传播给网络的参数
- 更新网络的权重，一般使用一个简单的规则：`weight = weight - learning_rate * gradient`

```python
import torch

class TwoLayerNet(torch.nn.Module):
    def __init__(self, D_in, H, D_out):
        """
        在构造函数中，我们实例化了两个nn.Linear模块，并将它们作为成员变量。
        """
        super(TwoLayerNet, self).__init__()
        self.linear1 = torch.nn.Linear(D_in, H)
        self.linear2 = torch.nn.Linear(H, D_out)

    def forward(self, x):
        """
        在前向传播的函数中，我们接收一个输入的张量，也必须返回一个输出张量。
        我们可以使用构造函数中定义的模块以及张量上的任意的（可微分的）操作。
        """
        h_relu = self.linear1(x).clamp(min=0)
        y_pred = self.linear2(h_relu)
        return y_pred

# N是批大小； D_in 是输入维度；H 是隐藏层维度； D_out 是输出维度
N, D_in, H, D_out = 64, 1000, 100, 10

# 产生输入和输出的随机张量
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# 通过实例化上面定义的类来构建我们的模型。
model = TwoLayerNet(D_in, H, D_out)
#模型的可学习参数 params = list(TwoLayerNet.parameters())

#model = torch.nn.Sequential(
          torch.nn.Linear(D_in, H),
          torch.nn.ReLU(),
          torch.nn.Linear(H, D_out),
        )

# 构造损失函数和优化器。
# SGD构造函数中对model.parameters()的调用，
# 将包含模型的一部分，即两个nn.Linear模块的可学习参数。
loss_fn = torch.nn.MSELoss(reduction='sum')
# 动量常取0.5，0.9，0.99，用于加速优化
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.5)
#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for t in range(500):
    # 前向传播：通过向模型传递x计算预测值y
    y_pred = model(x)

    #计算并输出loss
    loss = loss_fn(y_pred, y)
    print(t, loss.item())

    # 清零梯度，反向传播，更新权重
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

定义 `forward` 函数，使用任何针对张量的操作和计算。

> torch.nn只支持小批量处理（mini-batches）。整个torch.nn包只支持小批量样本的输入，不支持单个样本。

 比如，`nn.Conv2d` 接受一个4维的张量，即nSamples x nChannels x Height x Width

 如果是一个单独的样本，只需要使用`input.unsqueeze(0)`来添加一个“假的”批大小维度。

数据处理
---
`torchvision`包：包含了针对Imagenet、CIFAR10、MNIST等常用数据集的数据加载器（data loaders），还有对图片数据变形的操作，即`torchvision.datasets`和`torch.utils.data.DataLoader`。

```python
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

dataiter = iter(trainloader)
images, labels = dataiter.next()
```
**batch_size常取2的幂数倍，方便矩阵计算**


---

参考：
- [Deep-Learning-with-PyTorch-A-60-Minute-Blitz-cn](https://github.com/bat67/Deep-Learning-with-PyTorch-A-60-Minute-Blitz-cn)
- [pytorch-examples-cn](https://github.com/bat67/pytorch-examples-cn)
