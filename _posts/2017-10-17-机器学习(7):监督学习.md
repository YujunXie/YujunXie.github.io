---
title: 机器学习(7):监督学习
key: 20171017
tags: 机器学习
---

> 监督学习：用一组已知类别的样本调整模型的参数，使其达到所要求性能的过程。

之前将监督学习分为分类问题和回归问题。

当输出变量Y取有限个离散值时，预测问题便成为**分类问题**；当输出变量Y随输入变量X的变化而变化时，预测问题成为**回归问题**。

我们先来看看回归问题。

**回归问题**的学习等价于*函数拟合*：选择一条函数曲线使其很好地你和已知数据且很好地预测未知数据。有一元回归和多元回归，有线性回归和非线性回归。

刚接触回归问题时，有一个叫做逻辑回归的学习方法让人很迷。因为它的名字带有回归二字，但是却是一种分类模型。原因是逻辑回归模型的*核心*仍是一个*回归函数*。下面我们来看看线性回归和逻辑回归到底是怎样的。


<!--more-->


- **线性回归**

线性回归主要用于连续值的预测。

![linear.jpg](https://i.loli.net/2018/08/20/5b7a679b41281.jpg)

可以看出，我们需要找到一个合适的线性模型，来匹配这些数据点。
 
	一元线性回归：y = ax + b
	多元线性回归分析：hθ(x) = θ0 + θ1x1 + ... + θnxn
	代价函数：J(θ0,θ1,...,θn) = 1/2m∑(hθ(x(i))−y(i)).^2

求出参数 θ 的方法就是我们之前讨论过的梯度下降算法。若出现过拟合的情况，可以通过减少特征数目和正则化的方法进行避免。

----------

- **逻辑回归**

逻辑回归主要用于解决**分类问题**。但是本质还是一个回归函数，相当于对线性回归做了个压缩，通过**单位阶跃函数**`sigmoid`将y 的阈值从`y∈(+∞,−∞)`压缩到`(0,1)`。这样将回归问题转变成了分类问题。

![logistic.png](https://i.loli.net/2018/08/20/5b7a679b52c5d.png)

sigmoid函数：

![sigmoid.png](https://i.loli.net/2018/08/20/5b7a679b56dfe.png)

sigmoid函数把任何连续的值映射到`[0,1]`之间，数越大越趋向于0，越小越趋近于1。`θTX=0`就相当于是1类和0类的决策边界。

代价函数：

![cost.png](https://i.loli.net/2018/08/20/5b7a679b53d91.png)

![c.png](https://i.loli.net/2018/08/20/5b7a679b3d283.png)

![o.png](https://i.loli.net/2018/08/20/5b7a679b542bb.png)

其中`hθ(x)`是一个概率值，`y=1`表示正样本，`y=0`表示负样本。当y是正样本时，如果给定的概率特别小（预测为负样本），损失就会很大；给定的概率很大（预测为正样本），损失就会接近0。

**多分类问题**：

1）一对一

一对一将N个类别两两配对，从而产生N(N-1)/2个分类任务。

2）一对多

将一个类别作为正样本，其他的都作为负样本。

3）多对多
