---
title: 深度学习(2)：卷积神经网络CNN
key: 20171204
tags: 深度学习
---

卷积神经网络主要应用于图像的识别和分类，通过这篇文章了解它的架构和原理。

> 局部连接，共享参数。


 - 原理

![屏幕快照 2017-12-03 09.12.54.png](https://i.loli.net/2018/08/22/5b7cd6deb9b6a.png)

人的视角：看到一张图像，观察到其所在场景，立马识别场景中存在的物体，并知道它们是什么，属于什么类别，还能理解这张图片中的行为，情绪等等。这些动作几乎在一秒内完成，我们自己根本察觉不到。

机器视角：输入一张图像，得到一大堆像素值，根据图像的分辨率和尺寸，得到的是一个32x32x3的数组，其中前面两个数字相乘代表图像的分辨率，范围从0到255，表示这个像素点的灰度值，后面的3表示图像的RGB通道数。表征像素的数值以一定方式排列，从而形成了一张图像。所以说图像的信息是由像素点表征的。

计算机实现：通过输入表征图像的数组，寻找诸如边缘和曲线之类的低级特征来分类图片，继而通过一系列卷积层级建构出更为抽象的概念，如 ，最后输出描述该图像属于某一特定分类的概率的数字（比如：80% 是猫、15% 是狗、5% 是鸟）。

CNN 的工作：输入一张图像，历经一系列卷积层、激活层、池化层和完全连接层，最终得到输出。

<!--more-->

 - 生物学连接

   > 介绍些背景。CNN 是从视觉皮层的生物学上获得启发的。视觉皮层有小部分细胞对特定部分的视觉区域敏感。Hubel 和 Wiesel 于 1962 年进行的一项有趣的试验详细说明了这一观点，他们验证出大脑中的一些个体神经细胞只有在特定方向的边缘存在时才能做出反应（即放电）。例如，一些神经元只对垂直边缘兴奋，另一些对水平或对角边缘兴奋。Hubel 和 Wisesl 发现所有这些神经元都以柱状结构的形式进行排列，而且一起工作才能产生视觉感知。这种一个系统中的特定组件有特定任务的观点（视觉皮层的神经元细胞寻找特定特征）在机器中同样适用，这就是 CNN 的基础。

----------

 - end-to-end 架构

![屏幕快照 2017-11-14 16.31.37.png](https://i.loli.net/2018/08/22/5b7cd6dd77ad0.png)

![cnn.jpg](https://i.loli.net/2018/08/22/5b7cd85083ec3.jpg)

![conv.jpg](https://i.loli.net/2018/08/22/5b7cd6d716da4.jpg)

 1. **卷积层**  Convolution Layer

	> 假设手电筒光可以覆盖 5 x 5 的区域，想象一下手电筒光照过输入图像的所有区域。在机器学习术语中，这束手电筒被叫做过滤器（filter，有时候也被称为神经元（neuron）或核（kernel）），被照过的区域被称为感受野（receptive field）。过滤器同样也是一个数组（其中的数字被称作权重或参数）。重点在于过滤器的深度必须与输入内容的深度相同（这样才能确保可以进行数学运算），因此过滤器大小为 5 x 5 x 3。

	假设输入内容为一个 32 x 32 x 3 的像素值数组，5x5x3的过滤器在图像上以步幅1滑动，过滤器中的值与图像中的像素值做点乘，得到一个值。当滑过所有位置后得到一个28 x 28 x 
 1的数组，称之为激活映射或特征映射。

	（当我们使用两个而不是一个 5 x 5 x 3 的过滤器时，输出总量将会变成28 x 28 x 2。采用的过滤器越多，空间维度（ spatial dimensions）保留得也就越好。）
	
	随着经过更多的卷积层后，会得到更为复杂的激活映射，从线条到图形，最终得到能够用来分类的特征。
	
	可视化卷积中过滤器的内容：
	
	![屏幕快照 2017-12-03 12.46.17.png](https://i.loli.net/2018/08/22/5b7cd6df9f82c.png)

	看下最表层的滤波器，通过反向传播，他们将自己调整为彩色片和边缘的斑点。在我们深入到其他卷积层时，这些滤波器在做之前的卷积层的输入的点积。所以，它们正在采集这些较小的彩色片或边缘，并通过这些小的彩色片和边缘做出较大的彩色片。每一层都是对于一张图片从最基础的边缘，不断到最复杂的图片自己本身。

	![part.jpg](https://i.loli.net/2018/08/22/5b7cd6d2f12fe.jpg)

 2. **池化层**  Pooling Layer

	也叫做下采样(down-sampling)。
	
	保持图像的纵深，用于减少表征的空间尺寸，常出现在卷积层或激活层之后。
	
	最常用的是最大池化，使用一个尺寸为 2x2，步幅为2的过滤器，输出每个子区域中最大的数：
	
	![屏幕快照 2017-12-03 10.53.44.png](https://i.loli.net/2018/08/22/5b7cd851d7b16.png)
	
	![pool.png](https://i.loli.net/2018/08/22/5b7cd6da6169b.png)
	
	其他还有平均池化，L2-norm池化。
	
	优点：
	
	- 保留主要特征并减少权重参数，减小计算成本。
	- 控制过拟合。

 3. **全连接层** Full-Connected Layer

	全连接就是前一层的每个单元与后一层的单元都相互连接。这一层的作用是“*分类*”。核心操作就是矩阵的向量乘积 `y = wx` 。
	
	> 如果说卷积层，池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。
	
	在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为`1x1`的卷积；而前层是卷积层的全连接层可以转化为卷积核为`hxw`的全局卷积，h和w分别为前层卷积结果的高和宽。如将所有全连接层替换为卷积的全卷积神经网络（FCN)。
	
	注意这里全连接和卷积操作所使用的参数是相同多的。卷积层代替全连接层的优势：
	
	1）卷积层和全连接层的唯一区别在于卷积层的神经元对输入是局部连接的, 并且同一个通道内不同神经元共享权值(weights).
	
	2）因此卷积层可以接受任意大小的输入。
	
	上图是全连接层，下图是卷积层。
	
	![s.png](https://i.loli.net/2018/08/22/5b7cd6dc9edf6.png)
	
	由于在网络中全连接层的参数最冗余，几乎占整个网络所有参数的80%，所以目前使用这种性能更优的方法替代：全局平均池化（GAP）（对每个特征图一整张图片进行全局均值池化，这样每张特征图都可以得到一个输出，在论文《Network in Network》中提出）。
	
	![NIN.png](https://i.loli.net/2018/08/22/5b7cd6db3193d.png)
	
	关于使用 `softmax` 输出分类结果：softmax是一种归一化操作，若输入是一个100维向量，则输出一个100维的向量表示输入图像属于每一类的概率，且所有的概率加起来等于1。
	
 4. 常用层

	1）**非线性层(激活函数层)** Non-Linear Layer
	
	在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作的系统引入 **非线性特征** 。
	
	常用的激活函数有`sigmoid`, `tahn`, `Relu`等。
	
	这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。
	
	2）**Dropout层**
	
	在训练过程中，每次更新参数时随机断开一定百分比的输入神经元。此机制将保证神经网络不会对训练样本过于匹配，这将帮助缓解过拟合问题。另外，Dropout层只能在训练中使用，而不能用于测试过程。

----------

- 其他

**参数共享和局部连通性**

参数共享：通过一张特定的特征图中所有神经元来共享权重。

局部连通性：每个神经只连接一个输入图像的子集（不像神经网络中的所有神经元都完全连接在一起）。
(这帮助减少了整个系统中的参数数量，让计算变得更有效。)

步幅stride：过滤器移动的距离；

填充padding：保留更多信息；

计算任意给定卷积层的输出的大小：

![屏幕快照 2017-12-03 11.18.52.png](https://i.loli.net/2018/08/22/5b7cd6d5d3052.png)

W：输入尺寸，O：输出尺寸，K：过滤器尺寸，P：填充，S：步幅


**fine-tuning**

使用已用于其他目标，预训练好模型的权重或者部分权重，作为初始值开始训练。

原因：
- 自己从头训练卷积神经网络容易出现问题。
- fine-tuning能很快收敛到一个较理想的状态。

做法：
- 复用相同层的权重，新定义层取随机权重初始值。
- 调大新定义层的学习率，调小复用层学习率。


**感受野**

卷积神经网络的每一层输出的特征图（Feature map）上的像素点在原图像上映射的区域大小。计算采用从深层到前层的方式计算，即先计算最深层在前一层上的感受野，然后逐渐反馈到第一层。

![field.png](https://i.loli.net/2018/08/22/5b7cd84eae08b.png)


**数据增强技术**

一种改变训练数据的数组表征而保持标签不变的方法。常使用的增强方法包括灰度变化、水平翻转、垂直翻转、随机编组、色值跳变、翻译、旋转等其他多种方法。

**总结**

> 1）我们将输入图像传递到第一个卷积层中，卷积后以激活图形式输出。图片在卷积层中过滤后的特征会被输出，并传递下去。
> 
> 2）每个过滤器都会给出不同的特征，以帮助进行正确的类预测。因为我们需要保证图像大小的一致，所以我们使用同样的填充（零填充），否则填充会被使用，因为它可以帮助减少特征的数量。
> 
> 3）随后加入池化层进一步减少参数的数量。
> 
> 4）在预测最终提出前，数据会经过多个卷积和池化层的处理。卷积层会帮助提取特征，越深的卷积神经网络会提取越具体的特征，越浅的网络提取越浅显的特征。
> 
> 5）如前所述，CNN 中的输出层是全连接层，其中来自其他层的输入在这里被平化和发送，以便将输出转换为网络所需的参数。
> 
> 6）随后输出层会产生输出，这些信息会互相比较排除错误。损失函数是全连接输出层计算的均方根损失。随后我们会计算梯度错误。
> 
> 7）错误会进行反向传播，以不断改进过滤器（权重）和偏差值。 一个训练周期由单次正向和反向传递完成。

**应用**

![cnn2.png](https://i.loli.net/2018/08/22/5b7cd6dc9dfa8.png)

----------

卷积神经网络就像是一个黑盒，输入一张图像，就能出来一个分类结果，内部的细节我们无从探知，如为什么会刚好提取到我们想要的特征等等。

所以神经网络很大程度上取决于你的数据类型。图像的大小、复杂度、图像处理任务的类型以及其他更多特征的不同都会造成数据的不同。对于你的数据集，想出如何选择超参数的一个方法是找到能创造出图像在合适尺度上抽象的正确组合。

有看到过对于深度学习网络的特征可视化研究，感兴趣的可以接着去了解。

参考资料：

[机器视角：长文揭秘图像处理和卷积神经网络架构](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650728746&idx=1&sn=61e9cb824501ec7c505eb464e8317915&chksm=871b2d54b06ca442bc049e97c97e117455fd31bd0fb0619be4592eebd0958c26e3c223bfbfe5&scene=21#wechat_redirect)

[从入门到精通：卷积神经网络初学者指南](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650717691&idx=2&sn=3f0b66aa9706aae1a30b01309aa0214c&scene=21#wechat_redirect)

[全连接层的作用是什么？](https://www.zhihu.com/question/41037974)
