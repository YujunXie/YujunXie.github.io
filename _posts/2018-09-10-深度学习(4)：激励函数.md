---
title: 深度学习(4)：激励函数
key: 20180910
tags: 深度学习
---

激励函数作为卷积神经网络中的非线性层（激励层），对卷积层的输出结果做**非线性映射**。这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。

![CNN.png](https://i.loli.net/2018/09/10/5b967367566c4.png)

通过激励函数`f`对每一层卷积层线性输出结果做非线性映射，神经网络就有可能学习到平滑的曲线来实现对**非线性数据**的处理。

常用的激活函数有`Sigmoid`, `Tahn`, `ReLU`,`Leaky ReLU`,`ELU`,`Maxout`等。

<!--more-->

- `Sigmoid`函数

函数公式：$ \sigma (x) = \frac{1}{1+e^-x} $

函数图形：
![sigmoid.png](https://i.loli.net/2018/09/10/5b9675e63c889.png)

`sigmoid`函数将数据约束到`(0,1)`范围。数值越大的数映射为1，数值越小的数映射为0。

如今不常用。缺点如下：

1. 梯度消失与梯度饱和

	利用反向传播更新参数时，根据链式法则会乘以它的导数，`Sigmoid`函数的导数最大值为0.25，所以在经过多层神经网络后，会快速衰减到0。 

	如果输入的是比较大或者比较小的数，结果接近于1或0，梯度接近于0，会产生饱和效应，导致神经元类似于死亡状态。 

2. 输出不是0均值

	这导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。产生的一个结果就是：如果数据进入神经元的时候是正的，那么计算出的梯度也会始终都是正的，这会导致模型训练的收敛速度变慢。（如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的）

3. 幂运算耗时

----

- `Tahn`函数

函数公式：


函数图形：
![Tahn.png](https://i.loli.net/2018/09/10/5b96776407734.png)

`Tahn`函数将数据约束到`(-1,1)`范围。但仍具有`sigmoid`函数的缺点。

---

- `ReLU`函数（线性修正单元）
函数公式：


函数图形：
![ReLU.png](https://i.loli.net/2018/09/10/5b96776495c01.png)

常用。

优点：

1. 解决了梯度消失问题。因为它的导数等于1或者就是0。
2. 计算速度非常快，只需要判断输入是否大于0。
3. 可以非常大程度地提升随机梯度下降的收敛速度。

缺点：

1. 输出不是`zero-centered`。
2. 脆弱的`Dead ReLU Problem`，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见。(2)学习速率太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将学习速率设置太大或使用adagrad等自动调节学习速率的算法。

---

- `Leaky ReLU`函数
函数公式：


函数图形：
![Leaky ReLU.png](https://i.loli.net/2018/09/10/5b9677671c32a.png)

人们为了解决`Dead ReLU Problem`，提出了将`ReLU`的前半段设为`αx`而非0，通常`α=0.01`。另外一种直观的想法是基于参数的方法，即`ParametricReLU:f(x)=max(αx,x)`，其中α可由方向传播算法学出来。理论上来讲，`Leaky ReLU`有`ReLU`的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明`Leaky ReLU`总是好于`ReLU`。

---

- `ELU`函数

函数图形公式：
![elu.png](https://i.loli.net/2018/09/10/5b967768453c3.png)

1. `ELU`在正值区间的值为x本身，这样减轻了梯度消失问题（`x>0`区间导数处处为1），这点跟`ReLU`、`Leaky ReLU`相似。而在负值区间，`ELU`在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性。
2. 不会有Dead ReLU问题。
3. 输出的均值接近0。

上图是`ReLU`、`LReLU`、`ELU`的曲线比较图。

它的一个小问题在于存在指数，计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。

---

- `Maxout`函数

函数公式：

1. 计算是线性的，不会饱和不会挂掉。
2. 多了好些参数。
3. 两条直线拼接。

---

一些经验：
> 不使用`sigmoid`。
> 
> 首先试`ReLU`，因为速度快，但是要注意学习速率的设置，小心挂掉。
> 
> 如果`ReLU`挂掉，请用`Leaky ReLU`或`Maxout`。
> 
> 某些情况下`tahn`结果不错，但是很少。
