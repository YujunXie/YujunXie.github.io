---
title: 机器学习(6):梯度下降
key: 20171007
tags: 机器学习
---

**梯度下降**算法(Gradient Descent)是一种最小化代价函数J的算法。

> 梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。

> 是求解无约束最优化问题的一种最常用的方法，有实现简单的优点。

> 是迭代算法，每一步需要求解目标函数的梯度向量。

<!--more-->

原理并不难，和函数求导的思想大同小异，在平面的某一点出发，寻找梯度最大，也就是导数值最大的方向移动，直到找到最低点。

![gradient.png](https://i.loli.net/2018/08/20/5b7a65341f041.png)

如图所示，不同的出发点可能会收敛到不同的最低点。

最小化的目标函数是J，那么我们需要找到合适的theta值使得代价函数最小化。

参数theta的更新算法就是重复下式，直到收敛：

![theta.png](https://i.loli.net/2018/08/20/5b7a6533bae0b.png)

j 表示特征 j。（注意每次更新时，所有thetaj**同时更新**）

学习速率 α 表示了梯度下降的速率，因此 α 如果太小，收敛极慢；太大，很容易错过最低点，从而收敛失败。如果 α 是合适的速率，那么每一次迭代后代价函数 J 都是在减小的，可以通过画图描点的方式观察 J 的收敛性从而判断是否应该减小 α 。

看看后面的偏导怎么求：

![converge.png](https://i.loli.net/2018/08/20/5b7a6533e87fe.png)

代入后（xj(i)表示第i组数据中的第j个特征值）：

![t.png](https://i.loli.net/2018/08/20/5b7a6533e612f.png)

----------

上式也可以称作批量(Batch)梯度下降算法，因为每一次theta的更新都需要计算所有的样本取值。但是当样本数目非常大时，这种更新就会非常慢。因此，还有一些其他的改进的梯度下降算法或是更新theta的算法。

 - 小批量梯度下降算法

 - 随机梯度下降算法/增量梯度下降算法
![random.png](https://i.loli.net/2018/08/20/5b7a6533f3662.png)

 - Conjugate gradient，BFGS，L-BFGS

	优点：1) 不需要选取 α.  2) 比梯度下降更快
	
	缺点：比较复杂

----------

为了使梯度下降算法收敛更快，通常我们会对数据进行一下预处理，如特征缩放或均值归一化。

 - **特征缩放**
当多个特征的范围差距过大时，代价函数形成的轮廓图会非常的倾斜，梯度下降收敛太慢。所以通过特征缩放将特征的范围控制在一定范围内，有利于梯度下降的收敛。

![feature.png](https://i.loli.net/2018/08/20/5b7a65342f211.png)

 - **均值归一化**

  `xi = (xi - u) / s`

其中，u 为均值，s 为标准差。

----------

还有一种求`theta`的方法叫做**正规方程组**，是用矩阵的方法求解。不需要选取学习速率，特征缩放和迭代，但是当特征数目非常大时收敛非常慢。
