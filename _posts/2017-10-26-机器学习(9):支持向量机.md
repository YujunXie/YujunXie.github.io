---
title: 机器学习(9):支持向量
key: 20171026
tags: 机器学习
---


支持向量机属于监督学习的一种非线性分类模型，相比于逻辑回归和神经网络，它为复杂的非线性问题提供了更为清晰的解决方法。

 - 原理
SVM通常也被称为最大间距(Margin)分类器。支持向量机的学习策略就是**间隔最大化**。
间隔：[函数间隔和几何间隔](https://www.zhihu.com/question/20466147)

![margin.jpg](https://i.loli.net/2018/08/20/5b7a6b2e52c8a.jpg)

SVM学习的基本思想是求解能够正确划分训练数据集并且**几何间距最大**的分离超平面。

支持向量：训练数据集的样本点中与分离超平面距离最近的样本实例。只有移动**支持向量**才能改变所求的解，决定分离超平面。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本所决定。这是因为实际上距离分离超平面较远的实例大多能被正确分类，离超平面越近的实例的分类就容易被误分类，所以支持向量机以依赖于这些支持向量的分类。

<!--more-->

 - 线性可分数据集

线性可分SVM相当于单层神经网络。可以找到唯一一个分离超平面：`wx + b = 0;`

`W: weight vectot,(w1,w2,w3...wn)`, n 是特征值的个数

`X`: 训练实例

`b`: `bias`

 - 线性不可分数据集（非线性）

数据集在空间中对应的向量不可被一个超平面区分开。

![non.png](https://i.loli.net/2018/08/20/5b7a6b2fe9774.png)

1. 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中。
2. 在这个高维度的空间中找一个线性的超平面来根据线性可分的情况处理。

![nonlinear.png](https://i.loli.net/2018/08/20/5b7a6b30530b5.png)                                       

如何选择合理的非线性转化把数据转到高纬度中？如何解决计算内积时算法复杂度非常高的问题？

我们利用核函数来取代计算非线性映射函数的内积，SVM通过**核函数**视图将非线性的问题的数据集转变为核空间中一个线性可分的数据集。（核技巧）

常用的核函数有多项式和函数，高斯核函数，字符串核函数等。

将核函数应用于SVM中的内积计算，求解最优化问题。

