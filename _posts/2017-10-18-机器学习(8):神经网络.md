---
title: 机器学习(8):神经网络
key: 20171018
tags: 机器学习
---

神经网络是一种常用于**分类**的机器学习方法，它尝试模拟人类的大脑进行学习。

机器学习中的神经网络是一种*人工神经网络*，类似于人类大脑中神经元通过突触连接的结构进行信息处理。

 - 我们先从最简单的单层神经网络模型开始：

![single.jpg](https://i.loli.net/2018/08/20/5b7a699b6895b.jpg)

我们有三个输入a1,a2,a3，连接到一个神经元上，每条连接上带有这个输入所占的权值 w1,w2,w3，输入与权值求线性加权和得到a*w，设定一个阈值b。

![output.png](https://i.loli.net/2018/08/20/5b7a699c3ea03.png)

由于`a*w+b`不能保证输出的连续性，最后通过一个**激活函数**如`sigmoid`处理后输出分类结果。
我们训练模型的目标就是找到合适的权值。

这个最简单的单层神经网络模型也可以称为感知机，感知机类似一个逻辑回归模型，可以做线性分类任务。相当于找到一个分离超平面，将特征空间分为两个部分，分别为正类与负类。

![Perceptron.png](https://i.loli.net/2018/08/20/5b7a699f06023.png)

单层神经网络只能做线性分类任务。

<!--more-->

 - **多层神经网络**：将多个单一的神经元联结在一起。

可以看到，多层神经网络一般分为三层：输入层L1，隐藏层L2和输出层L3。

![network.png](https://i.loli.net/2018/08/20/5b7a699ea4796.png)

![hypothesis.png](https://i.loli.net/2018/08/20/5b7a69a166152.png)

为了方便矩阵运算，输入层和隐藏层我们会增加一个偏置单元。我们将上面的计算步骤称为**前向传播**：前一层的输出作为后一层的输入直至得到最后一个输出h。

中间可以增加更多的隐藏层，形成多层神经网络。

多层神经网络含有多个隐藏层，形成的分割超平面就相当于一个超曲面，因此可以对非线性的数据分类。
正是因为神经网络的非线性分类处理能力（拟合非线性数据模型），神经网络如今发展得如火如荼。

----------

 - 神经网络的学习

同样的，通过最小化代价函数得到模型的最优参数。

代价函数：

~~~
m = number of examples.
L = total number of layers in the network.
sl = number of units (not counting bias unit) in layer l.
K = number of output units/classes.
~~~

![cost.png](https://i.loli.net/2018/08/20/5b7a699e56eb1.png)

1. the double sum simply adds up the logistic regression costs calculated for each cell in the output layer.
2. the triple sum simply adds up the squares of all the individual Θs in the entire network.
3. the i in the triple sum does not refer to training example i.

梯度下降最小化代价函数：

![back.png](https://i.loli.net/2018/08/20/5b7a69a1671ce.png)

因为神经网络模型较为复杂，使用梯度下降算法计算每次梯度的代价很大。

我们使用**反向传播算法**：

~~~
（1）前向传播：将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果。
（2）计算误差，反向传播：由于神经网络的输出结果与实际结果有误差，
	则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
（3）更新权值：在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。
~~~

> 反向传播让我们能够训练网络“深处”的参数。

![backal.png](https://i.loli.net/2018/08/20/5b7a69a166a75.png)

其中误差的反向传递与**链式法则**有关，从后向前，不断将误差分摊到每个参数上。想进一步了解的同学可以看看这篇推导：[弄懂BP神经网络](http://www.cnblogs.com/charlotte77/p/5629865.html)

----------

> “时势造英雄”，神经网络这个概念很早就出来了，但是中间经历了几度冰封期，原因是当时的计算能力没能跟上来，制约了神经网络的计算，也就显示不出它的重要性。
